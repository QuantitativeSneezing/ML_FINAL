{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df10054",
   "metadata": {},
   "source": [
    "I'm submitting this particular notebook as my \"main notebook\", but please go to <link>https://github.com/QuantitativeSneezing/ML_FINAL</link> for the other notebooks containing the code for data cleanup and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8096b",
   "metadata": {},
   "source": [
    "Ever since Harry Markowitz' landmark portfolio theory paper in 1952, people have sought to apply quantitative methods to the stock market to achieve returns. The long history of the quantitative approach to investing means that there's a pretty extensive range of explanatory (and non explanatory) variables readily available, meaning it's a pretty fertile ground for training an AI with. Aside from the ease of data access, the stock market gives us another easy thing- a benchmark which we can measure against. On average, the market has gained roughly 10% a year, which means that a goal of being able to identify stocks with a greater return than that is a pretty obvious one. <br><br> Naturally, if it was as easy as feeding data to an AI model, we'd all by millionaires by now, but the market is notoriously difficult to predict, so a more reasonable goal will be choosing stocks which will outperform at a rate at least better than true randomness (50%). Outside of the model's performance, I'm also aiming to better wrangle time series data (which stock prices naturally are), as well as get some hands on experience working with neural networks, which seem to be quite popular these days.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b79a4",
   "metadata": {},
   "source": [
    "Before I can do any of that though, I'll need to clean up and organize the data, which I've retrieved from <br><br> (Oleh Onyshchak. (2020). Stock Market Dataset [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/1054465), <br><br>  which I'll be doing in the EDA notebook, so head there for a writeup on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e0f71",
   "metadata": {},
   "source": [
    "So, cleanup was quite the doozy, and I'll admit freely that most of the visualizations were subpar, but now we have our cleaned and prepared dataset, and it's time to build our model! <br> <br> I'm using pytorch for this, in the project.py file, and I'm going to make a multilayer perceptron. Of the neural net models, a recursive or convolutional one may be more technically suited for the task, but an MLP has two key features that make it attractive for this project in particular: first, they're computationally cheap- they're only a few layers of perceptrons, which makes it much easier to train, second, although it is a much simpler model, it still overlaps with many of the same hyperparameters, like bias and dropout. <br><br>As a quick refresher, (my understanding is that) a perceptron is *like* a logistic regressor in that it is a binary classifier, but it is unlike a logistic regressor it is a pure binary classification instead of a sigmoid. Specifically, perceptrons use a vector dot product to make their decisions, and only learn when wrong (with how much they learn determined by the learning rate). So when we make a multilayer perceptron, this really is another kind of ensemble/batching algorithm, just different than before because these can be modified into other neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8c1f4",
   "metadata": {},
   "source": [
    "From a rubric perspective, mlps also have a few things going for them- first, like all neural nets, they are resistant to multicollinearity (https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b4a29ccf31355a07ef60b4e223eb10e30cc29804, page 5 for a reference point), although this does come with the trade off of being difficult to intepret to the point that most neural nets are essentially black boxes. It's also not covered in class (obviously), and finally, hyperparameter tuning is (relatively) easy thanks to the Tune library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82002b23",
   "metadata": {},
   "source": [
    "So, now that the actual model is done, it's time for conclusions: The model *technically* is better than guessing for predicting that something will or won't beat the market, but this is mainly because it always predicts that it won't and that is the statistically likely outcome, so not exactly a rousing success. In terms of the specific outputs, it appears the tensor does change in value depending on the input, so at least the model has made some predictive changes (previously they were uniform, but that appears to be a result of my kernel crashing locally). Two big likely sources of improvement are increasing the epoch count and/or trying to upsample the data. Some external research also suggests that neural nets in general aren't amazing with unbalanced datasets, so the fact that 2/3 of the data in this binary classifier are one outcome might also be a strong influence. I'm running a higher epoch in the background as I write this, but it seems unlikely to get finished, so going to have to end with the conclusion that the model isn't performing as expected and only speculation on improvement. Will also mention that I didn't get to do the hyperparameter tuning I wanted for similar reasons- it would simply be too resource and time costly to be technically feasible at this point in time, sadly. After running the higher epochs (9), it appears the problem might lie in the sampling, which is definitely a lesson for another time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231990ad",
   "metadata": {},
   "source": [
    "Takeaways from the project as a whole: <br> Data wise, as is obvious, selecting datasets well is important. While the dataset looks impressive at a glance, with the sheer amount of data available, looking at the variables more closely would have been helpful- 5 out of 6 of the variables associated with each individual ticker's CSV files are *intensely* correlated, and the metadata file is pretty devoid of explanatory variables. The net result is this dataset is quite large, but also arguably quite shallow, with very little real info beyond how much a stock cost on a given day and how much it sold on that same day. If I had to do this again, I'd definitely be more discerning with how I select my dataset. Admittedly though, the subject matter itself is also pretty in need of normalization- a lot of workarounds needed to be made to deal with missing days or trying to deal with the large gaps in available time between tickers. Good to get hands on experience with time series data, though. Model wise, picking a neural net was a bit ambitious- I'd never worked with one before and didn't anticipate how long it would train even simple ones, meaning I ran out of time before I got the chance to perform the hyperparameter tuning I wanted to, since each model took 12 minutes to train, even cutting down epochs to 3 from the 10 I'd seen recommended. That said, failure is still learning, and now I have a better idea of the challenges associated with that kind of modelling, which is nice. <br> <br> In terms of takeaways for the future, one thing I'd really emphasize is goal- dataset alignment. The dataset would likely be a good match for a more short term prediction, since the time period between entries is quite short, and price and volume are the only real explanatory variables in the set. Conversely, something more longer term would probably be more suited to a dataset that has more explanatory variables, like one oriented on stock fundamentals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb92413",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eb59f66",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
