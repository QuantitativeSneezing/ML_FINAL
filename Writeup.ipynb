{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df10054",
   "metadata": {},
   "source": [
    "I'm submitting this particular notebook as my \"main notebook\", but please go to <link>https://github.com/QuantitativeSneezing/ML_FINAL</link> for the other notebooks containing the code for data cleanup and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8096b",
   "metadata": {},
   "source": [
    "Ever since Harry Markowitz' landmark portfolio theory paper in 1952, people have sought to apply quantitative methods to the stock market to achieve returns. The long history of the quantitative approach to investing means that there's a pretty extensive range of explanatory (and non explanatory) variables readily available, meaning it's a pretty fertile ground for training an AI with. Aside from the ease of data access, the stock market gives us another easy thing- a benchmark which we can measure against. On average, the market has gained roughly 10% a year, which means that a goal of being able to identify stocks with a greater return than that is a pretty obvious one. <br><br> Naturally, if it was as easy as feeding data to an AI model, we'd all by millionaires by now, but the market is notoriously difficult to predict, so a more reasonable goal will be choosing stocks which will outperform at a rate at least better than true randomness (50%). Outside of the model's performance, I'm also aiming to better wrangle time series data (which stock prices naturally are), as well as get some hands on experience working with neural networks, which seem to be quite popular these days.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b79a4",
   "metadata": {},
   "source": [
    "Before I can do any of that though, I'll need to clean up and organize the data, which I've retrieved from <br><br> (Oleh Onyshchak. (2020). Stock Market Dataset [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/1054465), <br><br>  which I'll be doing in the EDA notebook, so head there for a writeup on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e0f71",
   "metadata": {},
   "source": [
    "So, cleanup was quite the doozy, and I'll admit freely that most of the visualizations were subpar, but now we have our cleaned and prepared dataset, and it's time to build our model! <br> <br> I'm using pytorch for this, in the project.py file, and I'm going to make a multilayer perceptron. Of the neural net models, a recursive or convolutional one may be more technically suited for the task, but an MLP has two key features that make it attractive for this project in particular: first, they're computationally cheap- they're only a few layers of perceptrons, which makes it much easier to train, second, although it is a much simpler model, it still overlaps with many of the same hyperparameters, like bias and dropout. <br><br>As a quick refresher, (my understanding is that) a perceptron is *like* a logistic regressor in that it is a binary classifier, but it is unlike a logistic regressor it is a pure binary classification instead of a sigmoid. Specifically, perceptrons use a vector dot product to make their decisions, and only learn when wrong (with how much they learn determined by the learning rate). So when we make a multilayer perceptron, this really is another kind of ensemble/batching algorithm, just different than before because these can be modified into other neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8c1f4",
   "metadata": {},
   "source": [
    "From a rubric perspective, mlps also have a few things going for them- first, like all neural nets, they are resistant to multicollinearity (https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b4a29ccf31355a07ef60b4e223eb10e30cc29804, page 5 for a reference point), although this does come with the trade off of being difficult to intepret to the point that most neural nets are essentially black boxes. It's also not covered in class (obviously), and finally, hyperparameter tuning is (relatively) easy thanks to the Tune library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82002b23",
   "metadata": {},
   "source": [
    "So, now that the actual model is done, it's time for conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231990ad",
   "metadata": {},
   "source": [
    "Takeaways from the project as a whole: <br> Data wise, as is obvious, selecting datasets well is important. While the dataset looks impressive at a glance, with the sheer amount of data available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb92413",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
